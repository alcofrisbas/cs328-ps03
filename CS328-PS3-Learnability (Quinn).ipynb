{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "In class, we've discussed Gold's theorem and PAC learnability. In the cell below, identify one assumption in Gold's theorem that differs from how language is actually learned, and one assumption that is similar. Explain your answers, citing evidence from the reading where relevant. <i>Note: Your difference should not be that you think people are learning rules for a language, since you'll consider that in the final prompt in this workbook.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest difference is that parents don't actively try to confuse their children by switching languages constantly. Another difference is that when parents talk to their children, the words have meanings (as opposed to just word patterns) and are often in context so additional metadata can be interpreted by the children. A similarity is that in all models, the child eventually learns the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">PAC learning makes different assumptions from Gold's theorem about language learnability. Identify one assumption that differs between PAC learning and Gold's Theorem. Overall, do the differences in assumptions behind the PAC analysis (versus Gold's Theorem) make this analysis more or less relevant to human acquisition of language? Explain your answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gold's theorem, the language is perfectly identified in the limit but for PAC, the language is approximately identified/learned after enough data points have been given. Anther difference is that Gold's theorem doesn't work for infinite languages but PAC does. Overall, this makes PAC learnability closer to human learning because after a while we are almost always right about a sentence but are rarely alway perfect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We discussed in class that half lines are PAC-learnable. The same argument came be made for learning concepts along one feature dimension. For example, I might want to learn when it's time to transplant my tomato plants, and the only feature that matters for transplant is how tall the tomato is: there's a range starting at height $h_{1}$ and going until height $h_{2}$ where I should transplant the tomato, and tomatos that are outside that range (too short or too tall) should not be transplated. Argue that the cass of concepts that could represent transplantable tomato plants is PAC-learnable. Your answer does not need to be as rigorous as a formal proof, but should be clear about what you need to show and justify how you can show it for this concept (this could include why it's analogous to the half line concept).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can cast this problem onto a half-line by taking the height of the plant and taking the absolute distance from the average of $h_1$ and $h_2$. Therefore it will end up with a range from 0 to $(h_1-h_2)/2$ where the hypothesis is fine and after that, it will be wrong. Therefore there is a half line that stretches from $(h_1-h_2)/2$ to $-\\infty$ where all heights in this zone are good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Certain other classes of concepts are not PAC-learnable. For example, I might try to learn what sort of plants my cat likes to eat. I know it's dependent only on the size of the plant's leaves, but her preferences don't necessarily fall in an interval: she might like to eat plants with leaves that are between 2 and 3 square inches as well as plants with leaves between 9 and 10 square inches, but not plants with leaves that are 8 square inches. Intuitively explain why this class of concepts isn't PAC-learnable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things that are PAC learnable need a number such that once that number is surpassed, a hypothesis is never again wrong. With intervals (especially multiple intervals), the concept cant be PAC learnable because it either is incorrect for the second interval or it is incorrect for the space inbetween the intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "One argument that you might be tempted to make about grammar learning is that people are learning rules for how to generate sentences rather than directly learning to classify sentences as grammatical or ungrammatical; for example, they might be learning a context free grammar. Thus, the arguments we discussed do not apply. Explain why this argument isn't enough to discount the learnability results that we discussed; you may want to think back to the diagram of the Chomsky hierarchy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most obvious reason this is false is that people can immediatly tell if a sentence is not gramatical without knowing specifically why the sentence is not gramatical. Additioanlly, the way we form learn to form sentences is not consistent with the compsky hierarchy. If we were to learn by itterativly getting better at forming rules, it makes sense to believe that the way we would form sentences as a child would be using lower tier language patterns and then eventually getting up higher tier languages. This isn't consistent at all with how children form sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
