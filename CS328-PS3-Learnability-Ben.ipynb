{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "In class, we've discussed Gold's theorem and PAC learnability. In the cell below, identify one assumption in Gold's theorem that differs from how language is actually learned, and one assumption that is similar. Explain your answers, citing evidence from the reading where relevant. <i>Note: Your difference should not be that you think people are learning rules for a language, since you'll consider that in the final prompt in this workbook.</i></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way that Gold's theorem for learning differs from how langauge is actually learned is the method of teaching. Parents don't go around talking to their children in random languages to try fool them. Also, Gold's theorem does not take into account the meaning behind words in a sentence. Either way, however, (as long as the language as is finite), both Gold's method and how language is actually learned will eventually learn the language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">PAC learning makes different assumptions from Gold's theorem about language learnability. Identify one assumption that differs between PAC learning and Gold's Theorem. Overall, do the differences in assumptions behind the PAC analysis (versus Gold's Theorem) make this analysis more or less relevant to human acquisition of language? Explain your answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big difference between PAC learning and Gold's idea of learning is that in PAC learning, one stops listening after a while, hence the 'probably' accurate correct. Gold's theory of learning on the other hand will constantly switch languages when one sentence doesn't fit the current language, which reflects Gold's approach's inability to learn even one infite language, even if it's in a set of Finite languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "We discussed in class that half lines are PAC-learnable. The same argument came be made for learning concepts along one feature dimension. For example, I might want to learn when it's time to transplant my tomato plants, and the only feature that matters for transplant is how tall the tomato is: there's a range starting at height $h_{1}$ and going until height $h_{2}$ where I should transplant the tomato, and tomatos that are outside that range (too short or too tall) should not be transplated. Argue that the cass of concepts that could represent transplantable tomato plants is PAC-learnable. Your answer does not need to be as rigorous as a formal proof, but should be clear about what you need to show and justify how you can show it for this concept (this could include why it's analogous to the half line concept).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The case of transplanting tomato plants based solely on height is PAC-learnable due to the range of acceptable heights. By transposing the problem to a range of numbers $n \\in \\mathbb{R}$, we effectively turn the problem in to the half-line problem, which is proven to be PAC-Learnable. One way to do this is to map the sizes of tomato plants to their absolute distance from the average of $h_1$ and $h_2$. Now, the problem is identical to the half-line problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Certain other classes of concepts are not PAC-learnable. For example, I might try to learn what sort of plants my cat likes to eat. I know it's dependent only on the size of the plant's leaves, but her preferences don't necessarily fall in an interval: she might like to eat plants with leaves that are between 2 and 3 square inches as well as plants with leaves between 9 and 10 square inches, but not plants with leaves that are 8 square inches. Intuitively explain why this class of concepts isn't PAC-learnable.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of what makes the half-line example Pac-Learnable is that it has a single threshold. This enables the hyphotheses to either fail or succeed over a single point. Let's say that we have a wider hypothesis range in the half-line example. As we guess more numbers, in the case of  half-line, guesses are ruled out, but in this case, a hypothesis range could cover both satisfactory regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "One argument that you might be tempted to make about grammar learning is that people are learning rules for how to generate sentences rather than directly learning to classify sentences as grammatical or ungrammatical; for example, they might be learning a context free grammar. Thus, the arguments we discussed do not apply. Explain why this argument isn't enough to discount the learnability results that we discussed; you may want to think back to the diagram of the Chomsky hierarchy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To start out with, in order to properly develop rules about a language, one needs to first be able to distinguish whether  or not a sentence is in a langauge. In addition, by classifying sentences instead of rules, people can bypass the idea of tiers in the Chomsky heirarchy; classification is more powerful rule generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
